{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c35c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers # I use this to load the pretrained BERT\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers as ppb # pytorch transformers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d262b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad5ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "from typing import Optional, Union\n",
    "import pandas as pd, numpy as np, torch\n",
    "from datasets import Dataset\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "\n",
    "VER=2\n",
    "# TRAIN WITH SUBSET OF 60K\n",
    "NUM_TRAIN_SAMPLES = 1_024\n",
    "# PARAMETER EFFICIENT FINE TUNING\n",
    "# PEFT REQUIRES 1XP100 GPU NOT 2XT4\n",
    "USE_PEFT = False\n",
    "# NUMBER OF LAYERS TO FREEZE \n",
    "# DEBERTA LARGE HAS TOTAL OF 24 LAYERS\n",
    "FREEZE_LAYERS = 18\n",
    "# BOOLEAN TO FREEZE EMBEDDINGS\n",
    "FREEZE_EMBEDDINGS = True\n",
    "# LENGTH OF CONTEXT PLUS QUESTION ANSWER\n",
    "MAX_INPUT = 256\n",
    "# HUGGING FACE MODEL\n",
    "MODEL = 'microsoft/deberta-v3-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2597fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U /kaggle/input/faiss-cpu-173/faiss_cpu-1.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "\n",
    "!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431deac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U /kaggle/working/sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076b0573",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b659074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \" Function to clean text and keep only relevant ones\"\n",
    "    \n",
    "    # Remove Emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                          u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                          u\"\\U00002702-\\U000027B0\"\n",
    "                          u\"\\U000024C2-\\U0001F251\"\n",
    "                          \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "\n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2b89cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to use cross validation to get unbiased performance of the model on the training data\n",
    "\n",
    "count_vectorizer = feature_extraction.text.CountVectorizer() # Using the common bag of words technique\n",
    "train_vectors = count_vectorizer.fit_transform(x_train)\n",
    "baseline_model =  linear_model.LogisticRegression()\n",
    "f1_scores = model_selection.cross_val_score(baseline_model, train_vectors, y_train, cv=3, scoring=\"f1\")\n",
    "accuracy_scores = model_selection.cross_val_score(baseline_model, train_vectors, y_train, cv=3, scoring=\"accuracy\")\n",
    "print(f\"Cross Validation Accuracy Scores:\", accuracy_scores)\n",
    "print(f\"Cross Validation Accuracy f1_score:\", f1_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc1c9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model.fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa2ec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the scores on the test:\n",
    "\n",
    "test_vectors = count_vectorizer.transform(x_test)\n",
    "baseline_predict_test = baseline_model.predict(test_vectors)\n",
    "print(\"Accuracy:\", accuracy_score(baseline_predict_test, y_test))\n",
    "print(\"F1_score:\", f1_score(baseline_predict_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a0f50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll create a dictionary to convert option names (A, B, C, D, E) into indices and back again\n",
    "options = 'ABCDE'\n",
    "indices = list(range(5))\n",
    "\n",
    "option_to_index = {option: index for option, index in zip(options, indices)}\n",
    "index_to_option = {index: option for option, index in zip(options, indices)}\n",
    "\n",
    "def preprocess(example):\n",
    "    # The AutoModelForMultipleChoice class expects a set of question/answer pairs\n",
    "    # so we'll copy our question 5 times before tokenizing\n",
    "    first_sentence = [example['prompt']] * 5\n",
    "    second_sentence = []\n",
    "    for option in options:\n",
    "        second_sentence.append(example[option])\n",
    "    # Our tokenizer will turn our text into token IDs BERT can understand\n",
    "    tokenized_example = tokenizer(first_sentence, second_sentence, truncation=True)\n",
    "    tokenized_example['label'] = option_to_index[example['answer']]\n",
    "    return tokenized_example\n",
    "\n",
    "tokenized_train_ds = train_ds.map(preprocess, batched=False, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c3bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a pretrained BERT model so that I can use it to extract richer features of my dataset:\n",
    "\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, \n",
    "                                                    'distilbert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c9f893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following datacollator (adapted from https://huggingface.co/docs/transformers/tasks/multiple_choice)\n",
    "# will dynamically pad our questions at batch-time so we don't have to make every question the length\n",
    "# of our longest question.\n",
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0]['input_ids'])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        \n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721fc8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The BERT model works only when all vectors have the same length. But because some tweets are longer than others, I have to making all \n",
    "#vectors the same length by adding zeroes to vectors with shorter lengths (this shouldn't affect the feautres of the sentence)\n",
    "\n",
    "\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values]) #padding with zeroes\n",
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20051ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the masks that hide 15% of the words as I explained in my report\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e5c6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, I pass the data to the model to extract the richer features (I will technically take the output of the last layer):\n",
    "\n",
    "input_ids = torch.tensor(padded) #.to(torch.int64)\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f42587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As I explained in the report, I only take the CLS(its index is 0) vector because it represents the entire sentence\n",
    "\n",
    "features = last_hidden_states[0][:,0,:].numpy() \n",
    "labels = kaggle_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70f84ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will use the tokenization script provided by google:\n",
    "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ff115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_encoder(texts, tokenizer, max_len=512):\n",
    "  # this function will take the text and the tokenizer provided by google and will turn them into the three input types: tokens(words), masks(15% of the words), \n",
    "                                                                                                                                            # segments(pairs of sentences)\n",
    "    all_tokens = [] # bag of words\n",
    "    all_masks = [] # masks which are words that will be hidden and the model have to predict as part of the training\n",
    "    all_segments = [] #pairs of sentences that will help the model in learning the sequence\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text) # Turning the text into tokens\n",
    "            \n",
    "        text = text[:max_len-2] # making sure that all sentences have the same length\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"] # adding the \"CLS\":classification and \"SEP\":sentence separation\n",
    "                                                      # This tells the model when sentence start and end. This will be the segment input that will train the model\n",
    "                                                      #on the sequence\n",
    "\n",
    "        pad_len = max_len - len(input_sequence) # Bert takes as input a padded array\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) #turning the each word into its corresponding ID in the Bert model\n",
    "\n",
    "        # For Bert to work, all tokens vectors should have the same length, so here I'm adding 0s to vectors with smaller length to ensure\n",
    "        # that all have the same length. Adding 0 doesn't affect the features because 0 is not an ID for any word in the BERT model\n",
    "        tokens += [0] * pad_len #\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da5afed",
   "metadata": {},
   "outputs": [],
   "source": [
    "option_to_index = {option: idx for idx, option in enumerate('ABCDE')}\n",
    "index_to_option = {v: k for k,v in option_to_index.items()}\n",
    "\n",
    "def preprocess(example):\n",
    "    first_sentence = [ \"[CLS] \" + example['context'] ] * 5\n",
    "    second_sentences = [\" #### \" + example['prompt'] + \" [SEP] \" + example[option] + \" [SEP]\" for option in 'ABCDE']\n",
    "    tokenized_example = tokenizer(first_sentence, second_sentences, truncation='only_first', \n",
    "                                  max_length=MAX_INPUT, add_special_tokens=False)\n",
    "    tokenized_example['label'] = option_to_index[example['answer']]\n",
    "    \n",
    "    return tokenized_example\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        label_name = 'label' if 'label' in features[0].keys() else 'labels'\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0]['input_ids'])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        \n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993dd79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_index = read_index(\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")\n",
    "# FAISS on WIKI articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bb70af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the prompts\n",
    "prompt_embeddings = model.encode(trn.prompt.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True).half()\n",
    "prompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n",
    "search_score, search_index = sentence_index.search(prompt_embeddings, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f3353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512): \n",
    "    # The Bert model is not mainly designed for classification, so I will add one layer to do classification\n",
    "\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\") #the tokens\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\") # the masks (hidden words)\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\") # the pairs of sentences\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids]) #extracting the features\n",
    "    clf_output = sequence_output[:, 0, :] # the BERT model by default returns a vector that represents each word besides one vector that represents all the sentence \n",
    "                                          # This is the vector that has the same index (0) as the \"CLS\" that I inputed above so I'm building classification with that vector\n",
    "                                          # and I will ignore the other vectors as they are used for other purposes\n",
    "\n",
    "    out = Dense(1, activation='sigmoid')(clf_output) # Since I have only two classes, I'm adding one layer with a sigmoid function to do the classification\n",
    "    \n",
    "    # Adding everything together:\n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out) \n",
    "    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    # I'm using the same paramters used in the origional paper: Adam's optimizer and binary crossentropy which will work on maximizing the likelihood \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a04493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "# loading the pretrained model:\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829ae286",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMultipleChoice.from_pretrained(MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92fdfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data to evaluate the model\n",
    "x_train, x_test, y_train, y_test = train_test_split(kaggle_train['text'], \n",
    "                                                    kaggle_train['target'], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29919bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing the data using the functions above\n",
    "\n",
    "train_input = text_encoder(x_train, tokenizer, max_len=160)\n",
    "test_input = text_encoder(x_test, tokenizer, max_len=160)\n",
    "train_labels = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2079470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally training the model\n",
    "\n",
    "model = build_model(bert_layer, max_len=160)\n",
    "model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=3,\n",
    "    batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90907674",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c49d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('submission_sample.csv')\n",
    "test_input = text_encoder(test['text'], tokenizer, max_len=160)\n",
    "\n",
    "\n",
    "test_pred = model.predict(test_input).round().astype(int)\n",
    "\n",
    "submission['target'] = test_pred.round().astype(int)\n",
    "submission.to_csv('submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bc7976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadae776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b5558d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a60c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3efd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c538f073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027441b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19a7f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6648fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965340c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d30e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5cc313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
